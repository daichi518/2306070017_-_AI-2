# app.py
import streamlit as st
import pandas as pd
from review_analysis.sentiment import analyze_text_sentences, simple_japanese_sentiment
from review_analysis.category import categorize_sentences_by_topic, TOPICS
from storage import init_storage, append_record, load_records
from datetime import datetime
import io
import json

st.set_page_config(page_title="é£²é£Ÿãƒ¬ãƒ“ãƒ¥ãƒ¼è§£æã‚¢ãƒ—ãƒª", layout="wide")
st.title("ğŸ½ï¸ é£²é£Ÿãƒ¬ãƒ“ãƒ¥ãƒ¼è§£æã‚¢ãƒ—ãƒª â€” ã‚«ãƒ†ã‚´ãƒªåˆ¥æ„Ÿæƒ…è§£æï¼ˆå­¦ç±:2306070017 ä¸­å¶‹å¤§åœ°ï¼‰")
st.caption("å…¥åŠ›ã•ã‚ŒãŸãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ã€Œå‘³ï¼æ¥å®¢ï¼é›°å›²æ°—ï¼å¾…ã¡æ™‚é–“ï¼ä¾¡æ ¼ï¼æ¸…æ½”æ„Ÿã€ã«åˆ†é¡ã—ã€ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®æ„Ÿæƒ…å‚¾å‘ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚APIã‚­ãƒ¼ä¸è¦ã€‚")

# åˆæœŸåŒ–ï¼ˆCSVç°¡æ˜“DBï¼‰
init_storage()

st.sidebar.header("æ“ä½œ")
mode = st.sidebar.radio("å…¥åŠ›æ–¹æ³•", ["ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›", "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆ.txt/.csvï¼‰", "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§å®Ÿé¨“"])
save_to_db = st.sidebar.checkbox("è§£æçµæœã‚’ä¿å­˜ï¼ˆCSVã«è¿½åŠ ï¼‰", value=True)
show_saved = st.sidebar.button("ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º")

st.markdown("---")
st.subheader("ãƒ¬ãƒ“ãƒ¥ãƒ¼å…¥åŠ›")

raw_texts = []

if mode == "ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›":
    txt = st.text_area("ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’1ä»¶ãšã¤æ”¹è¡Œã§å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šç¾å‘³ã—ã‹ã£ãŸï¼ã§ã‚‚æ¥å®¢ãŒé…ã‹ã£ãŸï¼‰", height=200)
    if txt.strip():
        raw_texts = [s.strip() for s in txt.splitlines() if s.strip()]

elif mode == "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆ.txt/.csvï¼‰":
    uploaded = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ (.txt / .csv)", type=["txt", "csv"])
    if uploaded:
        if uploaded.name.lower().endswith(".txt"):
            content = uploaded.read().decode("utf-8", errors="ignore")
            raw_texts = [s.strip() for s in content.splitlines() if s.strip()]
        else:
            df = pd.read_csv(uploaded, dtype=str, keep_default_na=False)
            # ãƒ†ã‚­ã‚¹ãƒˆã‚‰ã—ãåˆ—ã‚’1ã¤è‡ªå‹•é¸æŠï¼ˆæœ€åˆã®objectåˆ—ï¼‰
            text_cols = [c for c in df.columns if df[c].dtype == object]
            if text_cols:
                col = text_cols[0]
            else:
                col = df.columns[0]
            raw_texts = [str(x).strip() for x in df[col].tolist() if str(x).strip()]

else:
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    sample = [
        "ãƒ‘ã‚¹ã‚¿ãŒã¨ã¦ã‚‚ãŠã„ã—ã‹ã£ãŸã€ã‚½ãƒ¼ã‚¹ãŒæ¿ƒåšã§æº€è¶³ã§ã™ã€‚",
        "ã‚¹ã‚¿ãƒƒãƒ•ã®å¯¾å¿œãŒé…ãã¦æ®‹å¿µã€‚æ³¨æ–‡ã‹ã‚‰æä¾›ã¾ã§æ™‚é–“ãŒã‹ã‹ã£ãŸã€‚",
        "åº—å†…ã¯è½ã¡ç€ã„ãŸé›°å›²æ°—ã§ãƒ‡ãƒ¼ãƒˆã«å‘ã„ã¦ã„ã‚‹ã¨æ€ã„ã¾ã™ã€‚",
        "å€¤æ®µãŒé«˜ã‚ã«æ„Ÿã˜ãŸãŒã€é‡ã¯ååˆ†ã ã£ãŸã€‚",
        "ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå°‘ã—æ±šã‚Œã¦ã„ãŸã€‚æ¸…æ½”æ„ŸãŒã‚‚ã†å°‘ã—æ¬²ã—ã„ã€‚"
    ]
    st.write("ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆç·¨é›†å¯ï¼‰")
    txt = st.text_area("", value="\n".join(sample), height=200)
    raw_texts = [s.strip() for s in txt.splitlines() if s.strip()]

if raw_texts:
    st.info(f"ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°: {len(raw_texts)} ä»¶")
    if st.button("è§£æé–‹å§‹"):
        # 1) æ–‡å˜ä½ã§åˆ†å‰²ãƒ»æ–‡ã”ã¨ã«æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ï¼ˆè‹±æ—¥å¯¾å¿œã®ç°¡æ˜“å®Ÿè£…ï¼‰
        sentences_info = analyze_text_sentences(raw_texts)  # list of dicts: {text, lang, scores}
        # 2) ã‚«ãƒ†ã‚´ãƒªå‰²å½“ï¼ˆå‘³ãƒ»æ¥å®¢ãªã©ï¼‰
        categorized = categorize_sentences_by_topic(sentences_info)

        # ã¾ã¨ã‚ï¼šã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ã‚¹ã‚³ã‚¢é›†è¨ˆ
        cat_rows = []
        for topic in TOPICS:
            sents = categorized.get(topic, [])
            if not sents:
                cat_rows.append({"topic": topic, "count": 0, "avg_compound": None, "positive_pct": None})
                continue
            comps = [s["scores"]["compound"] for s in sents]
            pos_cnt = sum(1 for s in sents if s["scores"]["compound"] > 0.05)
            avg = sum(comps)/len(comps)
            cat_rows.append({"topic": topic, "count": len(sents), "avg_compound": round(avg, 3), "positive_pct": round(pos_cnt/len(sents)*100, 1)})

        df_summary = pd.DataFrame(cat_rows)
        st.subheader("ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆï¼ˆscore: compoundï¼‰")
        st.dataframe(df_summary)

        # è©³ç´°è¡¨ç¤ºã‚¨ãƒªã‚¢
        st.subheader("æ–‡ã”ã¨ã®è§£æçµæœï¼ˆã‚«ãƒ†ã‚´ãƒªï¼è¨€èªï¼æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ï¼‰")
        detail_rows = []
        for topic, sents in categorized.items():
            for s in sents:
                detail_rows.append({
                    "category": topic,
                    "sentence": s["text"],
                    "lang": s.get("lang", "ja"),
                    "compound": s["scores"]["compound"],
                    "pos": s["scores"]["pos"],
                    "neu": s["scores"]["neu"],
                    "neg": s["scores"]["neg"]
                })
        df_detail = pd.DataFrame(detail_rows)
        st.dataframe(df_detail)

        # ä¿å­˜
        if save_to_db:
            rec = {
                "timestamp": datetime.utcnow().isoformat(),
                "num_reviews": len(raw_texts),
                "summary": df_summary.to_dict(orient="records"),
                "details": df_detail.to_dict(orient="records")
            }
            append_record(rec)
            st.success("è§£æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸï¼ˆdata/results.csvï¼‰")

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆJSONï¼‰
        buf = io.BytesIO(json.dumps({"summary": cat_rows, "details": detail_rows}, ensure_ascii=False, indent=2).encode("utf-8"))
        st.download_button("è§£æçµæœã‚’JSONã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=buf, file_name="review_analysis.json")

# ä¿å­˜ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
if show_saved:
    df_saved = load_records()
    st.subheader("ä¿å­˜æ¸ˆã¿ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆæœ€æ–°100ä»¶ï¼‰")
    st.dataframe(df_saved.tail(100))
